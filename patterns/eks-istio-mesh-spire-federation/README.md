# Istio Service Mesh with Spiffe/Spire Federation between EKS clusters

This pattern covers essential aspects of how to use Spiffe/Spire and Istio to bridge trust between multi-cluster Kubernetes service meshes. This allows microservices in different clusters to trust each other and communicate securely. You can also see [Istio Day talk](https://www.youtube.com/watch?v=FIVmVIJlLVw) on this topic.

## Introduction
 Modern architectures have a "web of complexity" with many interconnected microservices, legacy monolithic applications, cloud functions etc. spanning multiple environments like public cloud, private cloud, on-premises. This distributed, heterogeneous architecture often exists across different teams, business units or even organizations who need to integrate their components. With services heavily dependent on network communication, there are serious security implications in ensuring trusted interactions between the right components. Simply reusing the existing disparate security identity models from each independent application/workload is not straightforward. Establishing trust across disparate components is challenging due to the need for constant alignment between platform, security and app teams on security models and threat models. Developers must continuously modify microservices to accommodate different identity models. Managing misconfigurations becomes difficult when juggling multiple security models across an intricate architecture. The crux of the challenge is how to secure this "web of complexity" and establish trust across the widely distributed, heterogeneous environments as they evolve.

This pattern will address the challenge by achieving unified secure workload identity and cross-cluster trust using Istio mesh federation combined with the Spire identity attestation system. The pattern show two independent Kubernetes clusters (`foo-eks-cluster` and `bar-eks-cluster`) with different root CAs generated by the cert-manager. In each cluster, spire is installed and acts as an intermediate CA for the workloads. Federation is enabled between the two clusters by exchanging their trust bundles during spire installation. This allows workloads from different clusters with different root CAs to communicate.

## Benefits for Istio Service Mesh with Spiffe/Spire federation

**Increased security:** By using Spiffe/Spire and Istio, you can ensure that only authorized workloads are able to communicate with each other. This helps to protect your microservices from unauthorized access and malicious attacks.

**Simplified management:** Spiffe/Spire and Istio provide a centralized way to manage trust policies for your microservices. This makes it easier to enforce security policies across multiple clusters.

**Improved scalability:** Spiffe/Spire and Istio are designed to scale to large deployments. This makes them a good choice for organizations that need to secure a large number of microservices.

![Istio Service Mesh with Spiffe/Spire federation](../../images/eks-istio-spire.png "Istio Service Mesh with Spiffe/Spire federation on Amazon EKS")

## Prerequisites

Ensure that you have installed the following tools locally:

1. [awscli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
2. [kubectl](https://kubernetes.io/docs/tasks/tools/)
3. [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)
4. [istioctl](https://istio.io/latest/docs/ops/diagnostic-tools/istioctl/)

```bash
## clone this repo and got this folder
cd istio-on-eks/patterns/eks-istio-mesh-spire-federation
```

## Federation between two EKS clusters

### Create the clusters
cert-manager is also installed as part of EKS Addons because it would act as the root CA for each cluster. After installing cert-manager, we also passed the self-signed CA for the root CA manager as we want every cluster to be an "independent trust domain" with its own self-signed root CA from cert-manager.

#### Create the VPCs for the clusters and the peering between them

```bash
cd terraform/0.vpc
terraform init
terraform apply --auto-approve
```

#### Create `foo-eks-cluster`

```bash
cd ../1.foo-eks
terraform init
terraform apply --auto-approve
```

#### Create `bar-eks-cluster`

```bash
cd ../2.bar-eks
terraform init
terraform apply --auto-approve
```

Go to home folder

### Install Spire on the clusters with federation
This [script](https://github.com/aws-samples/istio-on-eks/blob/main/patterns/eks-istio-mesh-spire-federation/spire/install-spire.sh#L29) automates the exchange of CA bundles between clusters for federation. The script does the following:
* In order for cert-manager to act as the root CA, you need to "add the cert-manager as a clusterRole" to granting the cert-manager deployment the necessary RBAC permissions to function as the cluster-wide root certificate authority by binding it to the clusterRole.
* Creating the necessary namespace for Spire components
* Applying the Spire server and agent manifests/YAML files
* Configuring the trust domain for the Spire server
* Enabling the federation mode for the Spire server. For federation, each Spire server needs to share its bundle with the other trust domains. So defining this endpoint allows the bundles to be securely exposed and retrieved by the federated partners.
* Populating the trust bundle from the other cluster to enable cross-cluster communication. The bundle endpoint refers to the URL that the Spire server uses to distribute the bundle (collection of root CA certificates) to downstream Spire components like agents and workload attestors.

```bash
./spire/install-spire.sh
```

### Install Istio on the clusters
For the Istio installation, we are using the Istio Operator to deploy Istio on the clusters. The sidecar injection webhook for Spire - This allows the Spire sidecar to be automatically injected into application pods for identity.

**Note**: You will need EKS API endpoints from terraform outputs

`istioctl` needs to be in the PATH

```bash
./istio/install-istio.sh <cluster_endpoint_foo> <cluster_endpoint_bar>
```

### Deploy `helloworld` on both clusters
Two separate deployments named "hello-world-v1" and "hello-world-v2" are created, one in each cluster (`foo-eks-cluster` and `bar-eks-cluster` respectively). Both these deployments are configured to use the same Kubernetes service name "hello-world". Additionally, a separate "sleep" deployment (just serving as a test client) is also created in `foo-eks-cluster`.

```bash
./examples/deploy-heloworld.sh
```

### Check if everything is installed 

```bash
kubectl get po -A --context="${CTX_CLUSTER1}
kubectl get po -A --context="${CTX_CLUSTER2}
```

You should see similar to below:

```bash
kubectl get po -A --context $CTX_CLUSTER1
NAMESPACE          NAME                                            READY   STATUS    RESTARTS        AGE
amazon-guardduty   aws-guardduty-agent-dd78x                       1/1     Running   6 (3h29m ago)   3h32m
amazon-guardduty   aws-guardduty-agent-v945w                       1/1     Running   0               3h32m
amazon-guardduty   aws-guardduty-agent-zg9mr                       1/1     Running   0               3h32m
cert-manager       cert-manager-55657857dd-zd8jl                   1/1     Running   0               3h28m
cert-manager       cert-manager-cainjector-7b5b5d4786-gcnmd        1/1     Running   0               3h28m
cert-manager       cert-manager-webhook-55fb5c9c88-wc9f9           1/1     Running   0               3h28m
helloworld         helloworld-v1-6bb5b589d6-54fbk                  2/2     Running   0               44s
istio-system       istio-eastwestgateway-6c585467fd-sxwpd          1/1     Running   0               42m
istio-system       istio-ingressgateway-66595464dd-wplqc           1/1     Running   0               42m
istio-system       istiod-fc9564898-mvnzx                          1/1     Running   0               42m
kube-system        aws-load-balancer-controller-57765c4b45-mqgj7   1/1     Running   0               3h33m
kube-system        aws-load-balancer-controller-57765c4b45-qr7r2   1/1     Running   0               3h33m
kube-system        aws-node-4948g                                  2/2     Running   0               3h27m
kube-system        aws-node-mfn2g                                  2/2     Running   0               3h27m
kube-system        aws-node-xzgpb                                  2/2     Running   0               3h27m
kube-system        coredns-86bbb5f9b5-bdqwj                        1/1     Running   0               3h27m
kube-system        coredns-86bbb5f9b5-m7b2g                        1/1     Running   0               3h27m
kube-system        ebs-csi-controller-54457b68b-7fkxj              6/6     Running   0               3h22m
kube-system        ebs-csi-controller-54457b68b-rglqg              6/6     Running   0               3h27m
kube-system        ebs-csi-node-4ccgg                              3/3     Running   0               3h27m
kube-system        ebs-csi-node-g9bnb                              3/3     Running   0               3h27m
kube-system        ebs-csi-node-pdxmz                              3/3     Running   0               3h27m
kube-system        kube-proxy-7nkj2                                1/1     Running   0               3h27m
kube-system        kube-proxy-gwxtb                                1/1     Running   0               3h27m
kube-system        kube-proxy-j5zp8                                1/1     Running   0               3h27m
sleep              sleep-86bfc4d596-pl72r                          2/2     Running   0               20s
spire              spire-agent-bkk8q                               3/3     Running   0               52m
spire              spire-agent-jb57z                               3/3     Running   0               52m
spire              spire-agent-r9j86                               3/3     Running   0               52m
spire              spire-server-0                                  2/2     Running   0               52m
```

```bash
kubectl get po -A --context $CTX_CLUSTER2
NAMESPACE          NAME                                            READY   STATUS    RESTARTS   AGE
amazon-guardduty   aws-guardduty-agent-42twx                       1/1     Running   0          3h10m
amazon-guardduty   aws-guardduty-agent-fxngx                       1/1     Running   0          3h10m
amazon-guardduty   aws-guardduty-agent-tsn55                       1/1     Running   0          3h10m
cert-manager       cert-manager-55657857dd-wcwjh                   1/1     Running   0          65m
cert-manager       cert-manager-cainjector-7b5b5d4786-wmq2g        1/1     Running   0          65m
cert-manager       cert-manager-webhook-55fb5c9c88-kn9kw           1/1     Running   0          65m
helloworld         helloworld-v2-7fd66fcfdc-w7l7l                  2/2     Running   0          38s
istio-system       istio-eastwestgateway-57d65dfc66-5qgwt          1/1     Running   0          41m
istio-system       istio-ingressgateway-85b7dbbfd8-92pzg           1/1     Running   0          41m
istio-system       istiod-64d84b9dff-pxg67                         1/1     Running   0          42m
kube-system        aws-load-balancer-controller-7466ccb95b-gbksz   1/1     Running   0          3h11m
kube-system        aws-load-balancer-controller-7466ccb95b-hb7l4   1/1     Running   0          3h11m
kube-system        aws-node-gdbtw                                  2/2     Running   0          64m
kube-system        aws-node-ndt5l                                  2/2     Running   0          64m
kube-system        aws-node-vd4x2                                  2/2     Running   0          64m
kube-system        coredns-86bbb5f9b5-tw8pq                        1/1     Running   0          64m
kube-system        coredns-86bbb5f9b5-vm8zd                        1/1     Running   0          64m
kube-system        ebs-csi-controller-c88bff885-6kt58              6/6     Running   0          64m
kube-system        ebs-csi-controller-c88bff885-nhqqm              6/6     Running   0          59m
kube-system        ebs-csi-node-7lc7n                              3/3     Running   0          64m
kube-system        ebs-csi-node-jqw5n                              3/3     Running   0          64m
kube-system        ebs-csi-node-jsdf6                              3/3     Running   0          64m
kube-system        kube-proxy-57w59                                1/1     Running   0          64m
kube-system        kube-proxy-pvzx2                                1/1     Running   0          64m
kube-system        kube-proxy-wzblc                                1/1     Running   0          64m
sleep              sleep-64cbcc4cd9-xrqvv                          2/2     Running   0          26s
spire              spire-agent-bh4zq                               3/3     Running   0          51m
spire              spire-agent-hvpt8                               3/3     Running   0          51m
spire              spire-agent-tcv27                               3/3     Running   0          51m
spire              spire-server-0                                  2/2     Running   0          51m
```

### Check federation for east-west traffic between clusters
From a sleep pod in `foo-eks-cluster`, curling the hello-world service receives responses from both v1 and v2 deployments, proving east-west gateway communication across clusters. This curl command is actually reaching the "hello-world" service in `foo-eks-cluster`. However, since federation is enabled between the two clusters, Spire has issued identities to the workloads in both clusters in a way that allows them to communicate seamlessly. So when the curl command is executed from the sleep pod, it receives responses from both the "hello-world-v1" deployment in `foo-eks-cluster` as well as the "hello-world-v2" deployment in `bar-eks-cluster`. This demonstrates that traffic is flowing freely across the east-west gateway between the two federated clusters, despite the workloads originating from different clusters with different root CAs. It proves that the federated Spire setup with exchanged trust bundles has successfully enabled secure mTLS communication for these workloads deployed across different clusters.

```bash
kubectl exec --context="${CTX_CLUSTER1}" -n sleep -c sleep \
    "$(kubectl get pod --context="${CTX_CLUSTER1}" -n sleep -l \
    app=sleep -o jsonpath='{.items[0].metadata.name}')" \
    -- sh -c "while :; do curl -sS helloworld.helloworld:5000/hello; sleep 1; done"
```
You should see this:

```bash
Hello version: v1, instance: helloworld-v1-6bb5b589d6-54fbk
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v1, instance: helloworld-v1-6bb5b589d6-54fbk
Hello version: v1, instance: helloworld-v1-6bb5b589d6-54fbk
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v1, instance: helloworld-v1-6bb5b589d6-54fbk
```

### Check federation for north-south traffic between clusters
Initially, both the "hello-world-v1" deployment in `foo-eks-cluster` and "hello-world-v2" deployment in `bar-eks-cluster`  were running. Now lets scale down the "hello-world-v1" deployment in `foo-eks-cluster` to 0 replicas. This means there are no pods running for this deployment in `foo-eks-cluster`.
Scale down to 0 the helloworld-v1. Lets also create a Gateway and a Virtual Service for `helloworld` on `bar-eks-cluster` for the "hello-world-v2" deployment. An Istio Gateway acts as a load balancer to handle incoming traffic from outside the mesh. The VirtualService configures the routing rules for this ingress traffic. So now, instead of curling the "hello-world" service directly, lets curls the Gateway URL associated with the VirtualService in `bar-eks-cluster`. This Gateway URL is essentially an entry point from outside the mesh (in this case, from `foo-eks-cluster`) to reach the "hello-world-v2" service in `bar-eks-cluster`. When this curl command is run from the sleep pod in `foo-eks-cluster`, it receives only the responses from the "hello-world-v2" deployment behind the Gateway/VirtualService in `bar-eks-cluster`.

```bash
kubectl -n helloworld scale deploy helloworld-v1 --context="${CTX_CLUSTER1}" --replicas 0

sleep 2

kubectl apply --context="${CTX_CLUSTER2}" \
    -f ./examples/helloworld-gateway.yaml -n helloworld

export INGRESS_NAME=istio-ingressgateway
export INGRESS_NS=istio-system

GATEWAY_URL=$(kubectl -n "$INGRESS_NS" --context="${CTX_CLUSTER2}" get service "$INGRESS_NAME" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
```

Check the service by calling the Virtual Service from the `foo-eks-cluster`

```bash
kubectl exec --context="${CTX_CLUSTER1}" -n sleep -c sleep \
    "$(kubectl get pod --context="${CTX_CLUSTER1}" -n sleep -l \
    app=sleep -o jsonpath='{.items[0].metadata.name}')" \
    -- sh -c "while :; do curl -s http://$GATEWAY_URL/hello; sleep 1; done"
```

You should see this:

```bash
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
Hello version: v2, instance: helloworld-v2-7fd66fcfdc-w7l7l
```

## View the certificate trust chain
We will deploy the bookinfo application to illustrate that the root CA is cert-manager and spire is the intermediate CA. With this we will state that:
* That the root CA being used is cert-manager, as set up initially.
* That spire is acting as the intermediate CA and issuing the workload identities/certificates.

### Deploy bookinfo app

```bash
./istio/deploy-bookinfo.sh
```

### View the certificate trust chain for the productpage pod

```bash
istioctl proxy-config secret deployment/productpage-v1 -o json | jq -r '.dynamicActiveSecrets[0].secret.tlsCertificate.certificateChain.inlineBytes' | base64 --decode > chain.pem
```

Open the `chain.pem` file with a text editor, and you will see two certificates. Save the two certificates in separate files and use the openssl command `openssl x509 -noout -text -in $FILE` to parse the certificate contents.

## Setting up Automatic Certificate Rotation

Modify the rotation period for istiod certificates from 60 days (1440 hours) to 30 days (720 hours), run the following command:

```bash
kubectl -f ./cert-manager/cert-rotation.yaml --context $CTX_CLUSTER1
```

Check `istiod` logs

```bash
kubectl logs -l app=istiod -n istio-system -f
```

## Cleanup

Uninstall Istio on both clusters

```bash
./istio/cleanup-istio.sh
```

Uninstall Spire on both clusters

```bash
./spire/cleanup-spire.sh
```

Uninstall EKS clusters

```bash
cd terraform/1.foo-eks
terraform destroy --auto-approve

cd ../2.bar-eks
terraform destroy --auto-approve

cd ../0.vpc
terraform destroy --auto-approve
```
